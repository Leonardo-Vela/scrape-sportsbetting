{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import calc_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    # Configure Chrome options for headless mode\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')  # Run Chrome in headless mode\n",
    "    # Create a new instance of the Chrome WebDriver\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    # Navigate to the URL\n",
    "    print(\"Preparing soup ...\")\n",
    "    driver.get(url)\n",
    "    # Retrieve page source code \n",
    "    page_source = driver.page_source\n",
    "    # Parse the page source using BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "    return soup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_snapshot(soup):\n",
    "    # Get all \"live\" Event <a><a> elements\n",
    "    event_tags = []\n",
    "    try:\n",
    "        live_events = soup.select_one('div.Program-styles-module-desktop')\n",
    "        event_tags = live_events.select('a.EventRow-styles-module-event-row')\n",
    "    except:\n",
    "        pass\n",
    "    print(\"Tasting soup ...\")\n",
    "    # Extract text content from each anchor tag\n",
    "    # match time: xth minute\n",
    "    # event name: \"team A_versus_team B\"\n",
    "    # timestamp: \n",
    "    # odds: \n",
    "    # probabilities:\n",
    "    # score:\n",
    "    #columns = ['primary_key', 'name_team_A', 'name_team_B', 'gametime', 'score_team_A', 'score_team_B', 'odd_win_team_A', 'odd_draw', 'odd_win_team_B']\n",
    "    data = []\n",
    "    if len(event_tags) != 0:\n",
    "        for event in event_tags:\n",
    "            try:\n",
    "                team_names= event.select('span.EventTeams-styles-module-team-title')\n",
    "                #print(team_names)\n",
    "                score_tags = event.select_one('div.EventScores-styles-module-scores')\n",
    "                score_list = [char for char in [score.get_text(strip=True) for score in  score_tags][-1]]\n",
    "                team_names_list = [name.get_text(strip=True) for name in  team_names]\n",
    "                event_id = str.replace(team_names_list[0] + \"_versus_\" + team_names_list[1], \" \", \"_\")\n",
    "                # Get current date and time\n",
    "                current_datetime = datetime.datetime.now()\n",
    "                current_date = datetime.date.today()\n",
    "                current_time = datetime.datetime.now().time().strftime(\"%H:%M:%S\")\n",
    "                # Format the current datetime as a string\n",
    "                datetime_string = current_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                snapshot_id = event_id + \"_time_\" + datetime_string \n",
    "                #if len(event.select('div.EventDateTime-styles-module-live-date'))> 0:\n",
    "                game_time_tag = event.select_one('div.EventDateTime-styles-module-info-cell-live')\n",
    "                #print(game_time_tag)\n",
    "                game_time = [game_time.get_text(strip=True) for game_time in  game_time_tag][0]\n",
    "                #else\n",
    "                odd_tags = event.select('div.EventOddGroup-styles-module-odd-group')\n",
    "                odds_list = [1.0 if x == \"\" else float(x.replace(\",\",\".\")) for x in [odd.get_text(strip=True) for odd in  odd_tags[0]]]\n",
    "                #print(odds_list)\n",
    "                probs_list = calc_probs(odds_list)\n",
    "                #print(probs_list)\n",
    "                \n",
    "                row_data = {\n",
    "                    'primary_key': snapshot_id,\n",
    "                    'match_key': event_id,\n",
    "                    'date': current_date,\n",
    "                    'time_of_day': current_time,\n",
    "                    'name_team_A': team_names_list[0],\n",
    "                    'name_team_B': team_names_list[1],\n",
    "                    'gametime': game_time,\n",
    "                    'score_team_A': score_list[0],\n",
    "                    'score_team_B': score_list[1],\n",
    "                    'odd_win_team_A': odds_list[0],\n",
    "                    'odd_draw': odds_list[1],\n",
    "                    'odd_win_team_B': odds_list[2],\n",
    "                    'prob_win_team_A': probs_list[0],\n",
    "                    'prob_draw': probs_list[1],\n",
    "                    'prob_win_team_B': probs_list[2],\n",
    "                    }\n",
    "                \n",
    "                data.append(row_data)\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing soup ...\n"
     ]
    }
   ],
   "source": [
    "url = \"https://sports.tipico.de/de/live/default\"\n",
    "\n",
    "soup = get_soup(url)\n",
    "\n",
    "scraped_data = []\n",
    "\n",
    "# Define the duration for which you want the script to run (in seconds)\n",
    "duration =   900 # Run for 1 hour (adjust as needed)\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Main loop to scrape data\n",
    "while time.time() - start_time < duration:\n",
    "    # Your scraping logic here\n",
    "    # For example, navigate to a webpage and scrape its content\n",
    "    soup = get_soup(url)\n",
    "    # Scraping code...\n",
    "    # Store the scraped data in a dictionary or list\n",
    "    snapshot_data = extract_snapshot(soup)\n",
    "    scraped_data.append(snapshot_data)\n",
    "    # Sleep for a certain interval before the next iteration\n",
    "    time.sleep(10)  # Sleep for 10 seconds between iterations\n",
    "\n",
    "# Close the web driver\n",
    "\n",
    "# Convert the scraped data to a pandas DataFrame\n",
    "#df = pd.DataFrame(scraped_data)\n",
    "\n",
    "op_df = pd.concat(scraped_data, ignore_index=True)\n",
    "\n",
    "print(op_df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "\n",
    "op_df.to_csv('scraped_data.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
